{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "B_Team_Complete_Approaches",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Sentiment Analysis of Movie Titles**\n",
        " Using the Bag of Words Meets Bag of Popcorn Kaggle Tutorial Competition\n",
        "\n",
        "**Mandey Brown, Sam Fortescue, Mark Gardner, Emma Hamilton**"
      ],
      "metadata": {
        "id": "4bs42OTZceGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primary Packages Utilized"
      ],
      "metadata": {
        "id": "QdeTTu75clB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#used to remove html tags\n",
        "from bs4 import BeautifulSoup   \n",
        "\n",
        "#for uploading files (if done using below code)\n",
        "import io\n",
        "\n",
        "#remove punctuation and numbers\n",
        "import re\n",
        "\n",
        "#natural language tool kit --> remove a stop word\n",
        "import nltk     \n",
        "\n",
        "#other packages\n",
        "import sklearn    \n",
        "import pandas as pd \n",
        "import numpy as np "
      ],
      "metadata": {
        "id": "ETK9YhukYMLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading in the Data"
      ],
      "metadata": {
        "id": "xnmDPkb3cqHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unlab_train = pd.read_csv(\"/path-to/unlabeledTrainData.tsv\", header=0, \\\n",
        "                    delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "lab_train = pd.read_csv(\"/path-to/labeledTrainData.tsv\", header=0, \\\n",
        "                    delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "test = pd.read_csv(\"/path-to/testData.tsv\", header=0, delimiter=\"\\t\", \\\n",
        "                   quoting=3 )"
      ],
      "metadata": {
        "id": "hspjivPEbNKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 Bag of Words"
      ],
      "metadata": {
        "id": "NbDGQ4SpZCDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('The first review is:')\n",
        "print(lab_train[\"review\"][0])"
      ],
      "metadata": {
        "id": "XG-yph-PcbHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f12d7230-9410-41ea-aac9-1e27e121eb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first review is:\n",
            "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download and import the stopwords from the natural language toolkit library\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#function to preprocess the data\n",
        "  #remove html tags\n",
        "  #remove stop words\n",
        "#converts the review to a string made of \"important\" words\n",
        "def review_to_words( raw_review ):\n",
        "    # 1. Remove HTML\n",
        "    review_text = BeautifulSoup(raw_review).get_text() \n",
        "    \n",
        "    # 2. Remove non-letters        \n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
        "    \n",
        "    # 3. Convert to lower case and split into individual words\n",
        "    words = letters_only.lower().split()                             \n",
        "    \n",
        "    # 4. Convert stop word to a set\n",
        "      # sets are faster to search\n",
        "    stops = set(stopwords.words(\"english\"))                  \n",
        "    \n",
        "    # 5. Remove stop words\n",
        "    meaningful_words = [w for w in words if not w in stops]   \n",
        "    \n",
        "    # 6. Join the words back into one string separated by space\n",
        "        # return the result.\n",
        "    return( \" \".join( meaningful_words ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-481T1BKlrTK",
        "outputId": "8b265187-1162-4e92-d44d-48fc2ca70c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use the previously defined function to clean all of the movie reviews\n",
        "print(\"Cleaning the labeled training set movie reviews...\")\n",
        "\n",
        "clean_train_reviews = []\n",
        "\n",
        "for i in range( 0, len(lab_train[\"review\"])):\n",
        "   clean_train_reviews.append( review_to_words( lab_train[\"review\"].iloc[i] ))\n",
        "\n",
        "print(\"Done Cleaning.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfYcYqUlckZw",
        "outputId": "154ad86d-a2ab-4d1d-f1af-4823587f922a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning the labeled training set movie reviews...\n",
            "Done Cleaning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the bag of words\n",
        "print(\"Creating the Bag-of-words...\\n\")\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize the \"CountVectorizer\"\n",
        "# CountVectorizor is used to transform the cleaned review\n",
        "# into a vector based on the frequency of each word that occurs\n",
        "\n",
        "#Note: this count vectorizor is limited to the first 5000 words per review\n",
        "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
        "                             tokenizer = None,    \\\n",
        "                             preprocessor = None, \\\n",
        "                             stop_words = None,   \\\n",
        "                             max_features = 5000)\n",
        "print(\"Bag-of-words Created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcjbNbeGc2cF",
        "outputId": "177a054e-205b-4766-9fdb-39482136f446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating the Bag-of-words...\n",
            "\n",
            "Bag-of-words Created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fit_transform() has two tasks: \n",
        "#First, it fits the model and learns the vocab\n",
        "#Second, it transforms training data into feature vectors\n",
        "    #feature vectors are used to represent the frequency of each word in each review\n",
        "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
        "np.array(train_data_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dePtefP6dGJI",
        "outputId": "40c9a8eb-aa7f-411d-e9c0-38a17025474a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(<25000x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 1975048 stored elements in Compressed Sparse Row format>,\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit the RF model using the labeled training set\n",
        "#the bag of words are the features \n",
        "#the sentiment labels are the response variable\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "print(\"Training the Random Forest (this may take a while)...\")\n",
        "\n",
        "forest = RandomForestClassifier(n_estimators = 100)\n",
        "forest = forest.fit( train_data_features, lab_train[\"sentiment\"] )\n",
        "\n",
        "print(\"Done training RF.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T163lrkne1GV",
        "outputId": "d2d0dff3-98a1-4522-bedf-b33d0a90161e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the Random Forest (this may take a while)...\n",
            "Done training RF.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#need to also clean the test data so that it can be\n",
        "#compared to the training data\n",
        "clean_test_reviews = []\n",
        "\n",
        "print(\"Cleaning the test set movie reviews...\")\n",
        "for i in range(0,len(test[\"review\"])):\n",
        "  clean_test_reviews.append( review_to_words( test[\"review\"].iloc[i] ))\n",
        "\n",
        "print(\"Done cleaning test data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xky41IzAf3Cy",
        "outputId": "decc5c0e-f5b4-4238-a753-29d021b31984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning the test set movie reviews...\n",
            "Done cleaning test data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#also get a bag of words for the test data\n",
        "#and convert to feature vectors\n",
        "#once again to be able to compare with the training data\n",
        "test_data_features = vectorizer.transform(clean_test_reviews)\n",
        "np.array(test_data_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrP31fkxgR9k",
        "outputId": "da4f0026-12e9-4cbe-b9dc-6a0fd0220b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(<25000x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 1927864 stored elements in Compressed Sparse Row format>,\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predict the sentiment scores now using the test data feature and the RF\n",
        "print(\"Predicting test labels...\")\n",
        "result = forest.predict(test_data_features)\n",
        "print(\"Predicted Sentiment scores calculated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf0C_lCigW8l",
        "outputId": "f0b2cd97-6a3f-42fe-f627-ec24f117487a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting test labels...\n",
            "Predicted Sentiment scores calculated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get an output to submit to the competition and gain insight into an accuracy \n",
        "#to compare with the other approaches\n",
        "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )"
      ],
      "metadata": {
        "id": "UcHzlou3go4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use pandas to write the comma-separated csv file\n",
        "#this csv will be submitted to the kaggle competition\n",
        "output.to_csv( \"Bag_of_Words_model.csv\", index=False, quoting=3 )\n",
        "print(\"CSV for Part 1 created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjQMIx3kfUwD",
        "outputId": "d85c935e-9e95-4851-9c05-7718a3aa2b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV for Part 1 created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 Word Vectors"
      ],
      "metadata": {
        "id": "O9oSNuSijGDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#utilizes the cleaning function from part1 with \n",
        "#the option of not removing the stopwords \n",
        "#(which is more ideal for Word2Vec b/c relies on broader context of sentences)\n",
        "\n",
        "#function will clean the data and convert review/sentence\n",
        "#into a list of words\n",
        "def review_to_wordlist( review, remove_stopwords=False ):\n",
        "    # 1. Remove HTML\n",
        "    review_text = BeautifulSoup(review).get_text()\n",
        "     \n",
        "    # 2. Remove non-letters (i.e numbers etc.)\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "    \n",
        "    # 3. Convert words to lower case and split them\n",
        "    words = review_text.lower().split()\n",
        "    \n",
        "    # 4. Remove stop words (less ideal for Word2Vec)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    \n",
        "    # 5. Return a list of words\n",
        "    return(words)"
      ],
      "metadata": {
        "id": "4hs4AFZ72YQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Word-2-Vec expects single sentence input\n",
        "    #each as a list of words\n",
        "#punkt tokenizer will account for variety of ways to end sentences in \n",
        "    #the english language (?, !, etc.)\n",
        "    #also does not rely in spacing and capitalization to do so\n",
        "\n",
        "#install and download that\n",
        "nltk.download('punkt') \n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfJS4gvKj0-a",
        "outputId": "1823352a-25f9-41cd-cca1-d68530058ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function to split a review into sentences\n",
        "\n",
        "#Note this will produce a warning about urls, this is not an issue\n",
        "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
        "    # 1. Use the NLTK tokenizer to split paragraph into sentences\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    \n",
        "    # 2. Loop over each sentence\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        # If a sentence is empty, skip it\n",
        "        if len(raw_sentence) > 0:\n",
        "            # Otherwise, call review_to_wordlist to get a list of words\n",
        "            sentences.append( review_to_wordlist( raw_sentence, remove_stopwords ))\n",
        "    \n",
        "    # Return the list of sentences (each sentence is a list of words\n",
        "        #returns a list of lists\n",
        "    return sentences\n",
        "\n",
        "sentences_lab = []  # Initialize an empty list of sentences\n",
        "sentences_unlab = []\n",
        "\n",
        "print(\"Parsing sentences from labeled training set\")\n",
        "for review in lab_train[\"review\"]:\n",
        "    sentences_lab += review_to_sentences(review, tokenizer)\n",
        "print(\"Finished parsing sentences in the labeled training set!\")\n",
        "\n",
        "print(\"Parsing sentences from unlabeled set\")\n",
        "for review in unlab_train[\"review\"]:\n",
        "    sentences_unlab += review_to_sentences(review, tokenizer)\n",
        "print(\"Finished parsing sentences in the unlabeled training set!\")"
      ],
      "metadata": {
        "id": "qWPwuDubkC5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logging will allow for Word2Vec to create a nice output message\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
        "        level=logging.INFO)\n",
        "\n",
        "# Set values for various parameters\n",
        "num_features = 300    # Word vector dimensionality\n",
        "min_word_count = 40   # Minimum word count\n",
        "num_workers = 4       # Number of threads to run in parallel\n",
        "context = 10          # Context window size\n",
        "downsampling = 1e-3   # Downsample setting for frequent words"
      ],
      "metadata": {
        "id": "U1kq0tL9l7p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize and train the Word2Vec model\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "print(\"Training Word2Vec model...\")\n",
        "model = Word2Vec(sentences_lab, workers=num_workers, \\\n",
        "                size=num_features, min_count = min_word_count, \\\n",
        "                window = context, sample = downsampling, seed=1)"
      ],
      "metadata": {
        "id": "D_IATnL5mARt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This makes the model more memory efficient\n",
        "model.init_sims(replace=True)\n",
        "\n",
        "# name the model to use in part 3\n",
        "model_name = \"word2vec_model\"\n",
        "model.save(model_name)"
      ],
      "metadata": {
        "id": "oDKVtyIhmS2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "695977c8-0815-453f-dc5c-e7c9b90ba959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-03-01 00:22:41,831 : INFO : precomputing L2-norms of word weight vectors\n",
            "2022-03-01 00:22:41,913 : INFO : saving Word2Vec object under word2vec_model, separately None\n",
            "2022-03-01 00:22:41,916 : INFO : not storing attribute vectors_norm\n",
            "2022-03-01 00:22:41,919 : INFO : not storing attribute cum_table\n",
            "2022-03-01 00:22:42,327 : INFO : saved word2vec_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 Clustering with Word2Vec"
      ],
      "metadata": {
        "id": "LgbBnXjrpekt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the model created in Part2\n",
        "model = Word2Vec.load(\"word2vec_model\")"
      ],
      "metadata": {
        "id": "hG83Rk89po-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bd1014b-41ff-49c4-e2b9-ec37ae5484d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-03-01 00:23:57,927 : INFO : loading Word2Vec object from word2vec_model\n",
            "2022-03-01 00:23:58,251 : INFO : loading wv recursively from word2vec_model.wv.* with mmap=None\n",
            "2022-03-01 00:23:58,255 : INFO : setting ignored attribute vectors_norm to None\n",
            "2022-03-01 00:23:58,258 : INFO : loading vocabulary recursively from word2vec_model.vocabulary.* with mmap=None\n",
            "2022-03-01 00:23:58,264 : INFO : loading trainables recursively from word2vec_model.trainables.* with mmap=None\n",
            "2022-03-01 00:23:58,266 : INFO : setting ignored attribute cum_table to None\n",
            "2022-03-01 00:23:58,268 : INFO : loaded word2vec_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word2Vec naturally creates clusters semantically related words\n",
        "#use a K-means approach to find the center of the word clusters\n",
        "\n",
        "#Note: This code does take a while to compute\n",
        "    #To see how long it might take a timer is set\n",
        "    #It usually takes around 20 minutes\n",
        "import time\n",
        "start = time.time() # Start time\n",
        "\n",
        "#Set \"k\" (or the number of clusters) to be 1/5th the vocab size\n",
        "    #This way there are about only 5 words per cluster\n",
        "      #This was found to be the most effective approach via the \n",
        "      #trial and error computed by the group who originally made\n",
        "      #the kaggle competition\n",
        "word_vectors = model.wv.syn0\n",
        "num_clusters = word_vectors.shape[0] // 5\n",
        "\n",
        "#Initialize a K-Means object\n",
        "  #use it to extract the centroids\n",
        "from sklearn.cluster import KMeans\n",
        "print(\"Running K means\")\n",
        "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
        "\n",
        "#store the cluster assignment for each word\n",
        "idx = kmeans_clustering.fit_predict( word_vectors )\n",
        "\n",
        "#see how long it took to compute the K-Means clustering\n",
        "end = time.time()\n",
        "elapsed = end - start\n",
        "print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11_jhwyLpsMQ",
        "outputId": "5df4c9e2-0b93-4805-b654-0ea5c551c04a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running K means\n",
            "Time taken for K Means clustering:  456.1618766784668 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#idx contains the cluster assignment for each word\n",
        "#index2word (from the model created in part 2) contains the vocab from that model\n",
        "\n",
        "#They are zipped into one dictionary\n",
        "word_centroid_map = dict(zip( model.wv.index2word, idx ))"
      ],
      "metadata": {
        "id": "XEP8UrzawB2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first ten clusters\n",
        "\n",
        "# Note: clusters can differ b/c Word2Vec relies on a random number seed\n",
        "for cluster in range(0,20):\n",
        "    # Print the cluster number\n",
        "    print(\"\\nCluster %d\" % cluster)\n",
        "        \n",
        "    # Find all of the words for that cluster number, and print them out\n",
        "    words = []\n",
        "    for i in range(0,len(word_centroid_map.values())):\n",
        "      if( list(word_centroid_map.values())[i] == cluster ): \n",
        "        words.append(list(word_centroid_map.keys())[i])\n",
        "    print(words)"
      ],
      "metadata": {
        "id": "rTeY8W49qtr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to convert reviews into bag-of-centroids\n",
        "  #similar to bag-of-words\n",
        "    #instead uses semantically related clusters instead of individual words\n",
        "\n",
        "#will return an array for each review\n",
        "  #each with a number of features equal to the number of clusters\n",
        "\n",
        "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
        "    # The number of clusters is equal to the highest cluster index\n",
        "    # in the word / centroid map\n",
        "    num_centroids = max( word_centroid_map.values() ) + 1\n",
        "    \n",
        "    # Pre-allocate the bag of centroids vector (for speed)\n",
        "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
        "    \n",
        "    # Loop over the words in the review. If the word is in the vocabulary,\n",
        "    # find which cluster it belongs to, and increment that cluster count\n",
        "    # by one\n",
        "    for word in wordlist:\n",
        "        if word in word_centroid_map:\n",
        "            index = word_centroid_map[word]\n",
        "            bag_of_centroids[index] += 1\n",
        "    \n",
        "    # Return the \"bag of centroids\"\n",
        "    return bag_of_centroids"
      ],
      "metadata": {
        "id": "WoE7S2W3pidI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating Bag-of-Centroids Training data\")\n",
        "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
        "train_centroids = np.zeros( (lab_train[\"review\"].size, num_clusters), dtype=\"float32\" )\n",
        "\n",
        "# Transform the training set reviews into bags of centroids\n",
        "counter = 0\n",
        "for review in clean_train_reviews:\n",
        "    train_centroids[counter] = create_bag_of_centroids( review, \\\n",
        "        word_centroid_map )\n",
        "    counter += 1\n",
        "\n",
        "print(\"Done Creating Bag-of-Centroids Training data\")"
      ],
      "metadata": {
        "id": "nCJqjw_8wtl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18429f46-affa-4525-fbce-6e239a618753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Bag-of-Centroids Training data\n",
            "Done Creating Bag-of-Centroids training data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Repeat for test reviews (so the data can be compares)\n",
        "print(\"Creating Bag-of-Centroids Test data\")\n",
        "\n",
        "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), \\\n",
        "        dtype=\"float32\" )\n",
        "\n",
        "counter = 0\n",
        "for review in clean_test_reviews:\n",
        "  test_centroids[counter] = create_bag_of_centroids( review, \\\n",
        "    word_centroid_map )\n",
        "  counter += 1\n",
        "\n",
        "print(\"Done Creating Bag-of-Centroids Test data\")"
      ],
      "metadata": {
        "id": "1b6NebDBxA03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0949fd48-5e48-4342-a8cb-8ef9781e7f73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Bag-of-Centroids Test data\n",
            "Done Creating Bag-of-Centroids training data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit a random forest and extract predictions\n",
        "forest = RandomForestClassifier(n_estimators = 100)\n",
        "\n",
        "# Fitting the forest may take a few minutes\n",
        "print(\"Fitting a Random Rorest to labeled training data\")\n",
        "forest = forest.fit(train_centroids, lab_train[\"sentiment\"])\n",
        "result = forest.predict(test_centroids)\n",
        "\n",
        "print(\"Done fitting the RF\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrycuUqNxJ_U",
        "outputId": "a296ed47-f332-41e4-a841-538b4bbcec9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting a Random Rorest to labeled training data\n",
            "Done fitting the RF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the test results\n",
        "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})"
      ],
      "metadata": {
        "id": "oOJ22f_-9Eki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a CSV file from the output/results that will be submitted to the kaggle competition\n",
        "output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )"
      ],
      "metadata": {
        "id": "PRckDo569IT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Exploration: Bag-of-Words"
      ],
      "metadata": {
        "id": "DqGkyWNK9jmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This approach expands upon the Bag-of-Words approach previously done. It will break up the training data to fit the model to allow for model accuracy tests to gain further insight into how well the Bag-of-Words model is at predicting the sentiment of a movie review"
      ],
      "metadata": {
        "id": "UCaCuTE3-PCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the labeled training data"
      ],
      "metadata": {
        "id": "2CAdMWGC-jRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split training data here \n",
        "#allows for later model accuracy\n",
        "X = lab_train.drop('sentiment', axis = 1)\n",
        "y = lab_train['sentiment']\n",
        "\n",
        "#to split the training data to do model accuracy\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=11)"
      ],
      "metadata": {
        "id": "i71483vD90br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning and Text Preprocessing"
      ],
      "metadata": {
        "id": "QvLkrQMN-m7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Note: this utilizes the review_to_words function defined in the Part 1 section\n",
        "num_reviews = X_train[\"review\"].size\n",
        "\n",
        "print(\"Cleaning and parsing the training set movie reviews...\\n\")\n",
        "\n",
        "clean_train_reviews = []\n",
        "\n",
        "for i in range( 0, num_reviews ):\n",
        "    # If the index is evenly divisible by 1000, print a message\n",
        "    if( (i+1)%1000 == 0 ):\n",
        "        print(\"Review %d of %d\\n\" % ( i+1, num_reviews ))                                                                    \n",
        "    clean_train_reviews.append( review_to_words( X_train[\"review\"].iloc[i] ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNWV6qKM9-8G",
        "outputId": "725d8796-19d2-4f12-8bd0-47a7d4bad84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning and parsing the training set movie reviews...\n",
            "\n",
            "Review 1000 of 15000\n",
            "\n",
            "Review 2000 of 15000\n",
            "\n",
            "Review 3000 of 15000\n",
            "\n",
            "Review 4000 of 15000\n",
            "\n",
            "Review 5000 of 15000\n",
            "\n",
            "Review 6000 of 15000\n",
            "\n",
            "Review 7000 of 15000\n",
            "\n",
            "Review 8000 of 15000\n",
            "\n",
            "Review 9000 of 15000\n",
            "\n",
            "Review 10000 of 15000\n",
            "\n",
            "Review 11000 of 15000\n",
            "\n",
            "Review 12000 of 15000\n",
            "\n",
            "Review 13000 of 15000\n",
            "\n",
            "Review 14000 of 15000\n",
            "\n",
            "Review 15000 of 15000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the Bag of Words"
      ],
      "metadata": {
        "id": "uCXYrMz3-vL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating the Bag-of-Words\")\n",
        "\n",
        "#Note: this utilizes the CountVectorizor (vectorizer) previously defined in Part 1\n",
        "\n",
        "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
        "train_data_features = train_data_features.toarray()\n",
        "\n",
        "print(\"Sucessfully created Bag-of-Words\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUosYYJi-BLZ",
        "outputId": "f88a05fa-58a4-46e2-897c-8315425d5e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating the Bag-of-Words\n",
            "Sucessfully created Bag-of-Words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print the vocabullary that the Bag-of-Words identified\n",
        "vocab = vectorizer.get_feature_names()\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeLPcJEV-DKh",
        "outputId": "a5fdd832-36ef-47f2-bca6-49f3e3489096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abandoned', 'abc', 'abilities', 'ability', 'able', 'abraham', 'abrupt', 'absence', 'absolute', 'absolutely', 'absurd', 'abuse', 'abusive', 'abysmal', 'academy', 'accent', 'accents', 'accept', 'acceptable', 'acceptance', 'accepted', 'access', 'accident', 'accidentally', 'accompanied', 'accomplished', 'according', 'account', 'accuracy', 'accurate', 'accused', 'achieve', 'achieved', 'achievement', 'acid', 'across', 'act', 'acted', 'acting', 'action', 'actions', 'active', 'activities', 'actor', 'actors', 'actress', 'actresses', 'acts', 'actual', 'actually', 'ad', 'adam', 'adams', 'adaptation', 'adapted', 'add', 'added', 'adding', 'addition', 'adds', 'adequate', 'admirable', 'admire', 'admit', 'admittedly', 'adolescent', 'adopted', 'adorable', 'adult', 'adults', 'advance', 'advanced', 'advantage', 'adventure', 'adventures', 'advertising', 'advice', 'advise', 'affair', 'affect', 'affected', 'afford', 'aforementioned', 'afraid', 'africa', 'african', 'afternoon', 'afterwards', 'age', 'aged', 'agent', 'agents', 'ages', 'aging', 'ago', 'agree', 'agreed', 'agrees', 'ah', 'ahead', 'aid', 'aids', 'aim', 'aimed', 'air', 'aired', 'airplane', 'airport', 'aka', 'al', 'alan', 'alas', 'albeit', 'albert', 'album', 'alcohol', 'alcoholic', 'alec', 'alert', 'alex', 'alexander', 'alice', 'alien', 'aliens', 'alike', 'alison', 'alive', 'allen', 'allow', 'allowed', 'allowing', 'allows', 'almost', 'alone', 'along', 'alongside', 'already', 'alright', 'also', 'although', 'altman', 'altogether', 'always', 'amanda', 'amateur', 'amateurish', 'amazed', 'amazing', 'amazingly', 'ambiguous', 'ambitious', 'america', 'american', 'americans', 'amitabh', 'among', 'amongst', 'amount', 'amounts', 'amusing', 'amy', 'analysis', 'ancient', 'anderson', 'andre', 'andrew', 'andrews', 'andy', 'angel', 'angeles', 'angels', 'anger', 'angle', 'angles', 'angry', 'animal', 'animals', 'animated', 'animation', 'anime', 'ann', 'anna', 'anne', 'annie', 'annoyed', 'annoying', 'another', 'answer', 'answers', 'anthony', 'anti', 'antics', 'antonioni', 'antwone', 'anybody', 'anymore', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'apartment', 'ape', 'apes', 'appalling', 'apparent', 'apparently', 'appeal', 'appealing', 'appear', 'appearance', 'appearances', 'appeared', 'appearing', 'appears', 'appreciate', 'appreciated', 'appreciation', 'approach', 'appropriate', 'april', 'area', 'areas', 'argue', 'argument', 'ariel', 'arm', 'armed', 'arms', 'army', 'arnold', 'around', 'arrested', 'arrival', 'arrive', 'arrived', 'arrives', 'arrogant', 'art', 'arthur', 'artificial', 'artist', 'artistic', 'artists', 'arts', 'ashamed', 'ashley', 'asian', 'aside', 'ask', 'asked', 'asking', 'asks', 'asleep', 'aspect', 'aspects', 'ass', 'assassin', 'assault', 'assigned', 'assistant', 'associated', 'assume', 'astaire', 'astonishing', 'atlantis', 'atmosphere', 'atmospheric', 'atrocious', 'attached', 'attack', 'attacked', 'attacks', 'attempt', 'attempted', 'attempting', 'attempts', 'attenborough', 'attend', 'attention', 'attitude', 'attitudes', 'attorney', 'attracted', 'attraction', 'attractive', 'audience', 'audiences', 'audio', 'aunt', 'austen', 'austin', 'australia', 'australian', 'authentic', 'author', 'authority', 'automatically', 'available', 'average', 'avoid', 'avoided', 'awake', 'award', 'awards', 'aware', 'away', 'awe', 'awesome', 'awful', 'awfully', 'awkward', 'babe', 'babies', 'baby', 'bacall', 'back', 'backdrop', 'background', 'backgrounds', 'backs', 'bacon', 'bad', 'badly', 'bag', 'baker', 'bakshi', 'balance', 'ball', 'ballet', 'balls', 'band', 'bang', 'bank', 'banned', 'bar', 'barbara', 'bare', 'barely', 'bargain', 'barrel', 'barry', 'base', 'baseball', 'based', 'basement', 'basic', 'basically', 'basinger', 'basis', 'basketball', 'bat', 'bath', 'bathroom', 'batman', 'battle', 'battles', 'bay', 'bbc', 'beach', 'bear', 'bears', 'beast', 'beat', 'beaten', 'beating', 'beats', 'beatty', 'beautiful', 'beautifully', 'beauty', 'became', 'become', 'becomes', 'becoming', 'bed', 'bedroom', 'beer', 'befriends', 'began', 'begin', 'beginning', 'begins', 'behave', 'behavior', 'behind', 'beings', 'belief', 'beliefs', 'believable', 'believe', 'believed', 'believes', 'believing', 'bell', 'belong', 'belongs', 'beloved', 'belushi', 'ben', 'beneath', 'benefit', 'berlin', 'besides', 'best', 'bet', 'bette', 'better', 'bettie', 'beyond', 'bible', 'big', 'bigger', 'biggest', 'biko', 'bill', 'billy', 'bin', 'bird', 'birds', 'birth', 'birthday', 'bit', 'bitch', 'bite', 'bits', 'bitter', 'bizarre', 'black', 'blacks', 'blade', 'blah', 'blair', 'blake', 'blame', 'bland', 'blank', 'blast', 'blatant', 'bleak', 'blend', 'blew', 'blind', 'blob', 'block', 'blockbuster', 'blond', 'blonde', 'blood', 'bloody', 'blow', 'blowing', 'blown', 'blows', 'blue', 'blues', 'blunt', 'bo', 'board', 'boat', 'bob', 'bobby', 'bodies', 'body', 'bold', 'boll', 'bollywood', 'bomb', 'bond', 'bone', 'bonus', 'book', 'books', 'boom', 'boot', 'border', 'bore', 'bored', 'boredom', 'boring', 'born', 'boss', 'bother', 'bothered', 'bottle', 'bottom', 'bought', 'bound', 'bourne', 'bowl', 'box', 'boxing', 'boy', 'boyfriend', 'boyle', 'boys', 'brad', 'brady', 'brain', 'brains', 'branagh', 'brand', 'brando', 'brave', 'brazil', 'bread', 'break', 'breaking', 'breaks', 'breasts', 'breath', 'breathtaking', 'breed', 'brenda', 'brian', 'bride', 'bridge', 'bridget', 'brief', 'briefly', 'bright', 'brilliance', 'brilliant', 'brilliantly', 'bring', 'bringing', 'brings', 'britain', 'british', 'broad', 'broadcast', 'broadway', 'broke', 'broken', 'brooklyn', 'brooks', 'brosnan', 'brother', 'brothers', 'brought', 'brown', 'bruce', 'brutal', 'brutality', 'brutally', 'bsg', 'buck', 'bucks', 'bud', 'buddies', 'buddy', 'budget', 'buff', 'buffalo', 'buffs', 'bug', 'bugs', 'build', 'building', 'buildings', 'builds', 'built', 'bull', 'bullet', 'bullets', 'bumbling', 'bunch', 'buried', 'burn', 'burned', 'burning', 'burns', 'burt', 'burton', 'bus', 'bush', 'business', 'businessman', 'buster', 'busy', 'butler', 'butt', 'button', 'buy', 'buying', 'bye', 'cabin', 'cable', 'cage', 'cagney', 'caine', 'cake', 'california', 'call', 'called', 'calling', 'calls', 'came', 'cameo', 'cameos', 'camera', 'cameras', 'cameron', 'camp', 'campbell', 'campy', 'canada', 'canadian', 'candy', 'cannibal', 'cannon', 'cannot', 'cant', 'canyon', 'capable', 'capital', 'captain', 'captivating', 'capture', 'captured', 'captures', 'capturing', 'car', 'card', 'cardboard', 'cards', 'care', 'cared', 'career', 'careers', 'careful', 'carefully', 'carell', 'cares', 'caring', 'carl', 'carla', 'carol', 'carpenter', 'carradine', 'carrey', 'carrie', 'carried', 'carries', 'carry', 'carrying', 'cars', 'carter', 'cartoon', 'cartoons', 'cary', 'case', 'cases', 'cash', 'cast', 'casting', 'castle', 'cat', 'catch', 'catches', 'catching', 'catchy', 'category', 'catherine', 'catholic', 'cats', 'cattle', 'caught', 'cause', 'caused', 'causes', 'causing', 'cave', 'cd', 'celebration', 'celebrity', 'cell', 'celluloid', 'center', 'centered', 'centers', 'central', 'century', 'certain', 'certainly', 'cg', 'cgi', 'chain', 'chair', 'challenge', 'challenged', 'challenges', 'challenging', 'chamberlain', 'chan', 'chance', 'chances', 'change', 'changed', 'changes', 'changing', 'channel', 'channels', 'chaos', 'chaplin', 'chapter', 'character', 'characterization', 'characters', 'charge', 'charged', 'charisma', 'charismatic', 'charles', 'charlie', 'charlotte', 'charm', 'charming', 'chase', 'chased', 'chases', 'chasing', 'chavez', 'che', 'cheap', 'cheated', 'cheating', 'check', 'checked', 'checking', 'cheek', 'cheese', 'cheesy', 'chemistry', 'chest', 'chicago', 'chick', 'chicks', 'chief', 'child', 'childhood', 'childish', 'children', 'chilling', 'chills', 'china', 'chinese', 'choice', 'choices', 'choose', 'choreographed', 'choreography', 'chose', 'chosen', 'chris', 'christ', 'christian', 'christians', 'christie', 'christine', 'christmas', 'christopher', 'christy', 'chuck', 'church', 'cia', 'cinderella', 'cinema', 'cinematic', 'cinematographer', 'cinematography', 'circle', 'circumstances', 'cities', 'citizen', 'city', 'civil', 'civilization', 'claim', 'claimed', 'claims', 'claire', 'clark', 'clarke', 'class', 'classes', 'classic', 'classical', 'classics', 'claude', 'clean', 'clear', 'clearly', 'clever', 'cleverly', 'clich', 'cliche', 'cliff', 'climactic', 'climax', 'clint', 'clip', 'clips', 'clock', 'close', 'closed', 'closely', 'closer', 'closest', 'closet', 'closing', 'clothes', 'clothing', 'clown', 'club', 'clue', 'clues', 'clumsy', 'co', 'coach', 'code', 'cody', 'coffee', 'coherent', 'cold', 'cole', 'collection', 'college', 'colonel', 'color', 'colorful', 'colors', 'colour', 'columbo', 'com', 'combat', 'combination', 'combine', 'combined', 'come', 'comedian', 'comedic', 'comedies', 'comedy', 'comes', 'comfort', 'comfortable', 'comic', 'comical', 'comics', 'coming', 'command', 'comment', 'commentary', 'commented', 'comments', 'commercial', 'commercials', 'commit', 'committed', 'common', 'communist', 'community', 'companies', 'companion', 'company', 'compare', 'compared', 'comparing', 'comparison', 'compassion', 'compelled', 'compelling', 'competent', 'competition', 'complain', 'complaining', 'complaint', 'complaints', 'complete', 'completely', 'complex', 'complexity', 'complicated', 'composed', 'composer', 'computer', 'con', 'conceived', 'concept', 'concern', 'concerned', 'concerning', 'concerns', 'concert', 'conclusion', 'condition', 'conditions', 'confidence', 'conflict', 'conflicts', 'confused', 'confusing', 'confusion', 'connect', 'connected', 'connection', 'connery', 'conscious', 'consequences', 'conservative', 'consider', 'considerable', 'considered', 'considering', 'consistent', 'consistently', 'consists', 'conspiracy', 'constant', 'constantly', 'constructed', 'construction', 'contact', 'contain', 'contained', 'contains', 'contemporary', 'content', 'contest', 'context', 'continue', 'continued', 'continues', 'continuity', 'contract', 'contrary', 'contrast', 'contrived', 'control', 'controversial', 'conventional', 'conversation', 'conversations', 'convey', 'convince', 'convinced', 'convincing', 'convincingly', 'convoluted', 'cook', 'cool', 'cooper', 'cop', 'cops', 'copy', 'core', 'corner', 'corny', 'corporate', 'corpse', 'correct', 'corrupt', 'corruption', 'cost', 'costs', 'costume', 'costumes', 'could', 'count', 'counter', 'countless', 'countries', 'country', 'countryside', 'couple', 'coupled', 'couples', 'courage', 'course', 'court', 'cousin', 'cover', 'covered', 'covers', 'cowboy', 'cox', 'crack', 'cracking', 'craft', 'crafted', 'craig', 'crap', 'crappy', 'crash', 'craven', 'crazy', 'cream', 'create', 'created', 'creates', 'creating', 'creation', 'creative', 'creativity', 'creator', 'creators', 'creature', 'creatures', 'credibility', 'credible', 'credit', 'credits', 'creep', 'creepy', 'crew', 'cried', 'crime', 'crimes', 'criminal', 'criminals', 'cringe', 'crisis', 'critic', 'critical', 'criticism', 'critics', 'crocodile', 'cross', 'crowd', 'crude', 'cruel', 'cruise', 'crush', 'cry', 'crying', 'crystal', 'cuba', 'cube', 'cue', 'cult', 'cultural', 'culture', 'cup', 'cure', 'curiosity', 'curious', 'current', 'currently', 'curse', 'curtis', 'cusack', 'cut', 'cute', 'cuts', 'cutting', 'cynical', 'da', 'dad', 'daddy', 'daily', 'dalton', 'damage', 'damme', 'damn', 'damon', 'dan', 'dana', 'dance', 'dancer', 'dancers', 'dances', 'dancing', 'danes', 'danger', 'dangerous', 'daniel', 'daniels', 'danny', 'dare', 'daring', 'dark', 'darker', 'darkness', 'darren', 'date', 'dated', 'dating', 'daughter', 'daughters', 'dave', 'david', 'davies', 'davis', 'dawn', 'dawson', 'day', 'days', 'de', 'dead', 'deadly', 'deaf', 'deal', 'dealing', 'deals', 'dealt', 'dean', 'dear', 'death', 'deaths', 'debut', 'decade', 'decades', 'decent', 'decide', 'decided', 'decides', 'decision', 'decisions', 'dedicated', 'dee', 'deep', 'deeper', 'deeply', 'defeat', 'defend', 'defense', 'defined', 'definite', 'definitely', 'degree', 'del', 'deleted', 'deliberately', 'delight', 'delightful', 'deliver', 'delivered', 'delivering', 'delivers', 'delivery', 'demand', 'demanding', 'demands', 'demented', 'demise', 'demon', 'demons', 'deniro', 'dennis', 'dentist', 'denzel', 'department', 'depicted', 'depicting', 'depiction', 'depicts', 'depressed', 'depressing', 'depression', 'depth', 'depths', 'der', 'deranged', 'derek', 'descent', 'describe', 'described', 'describes', 'description', 'desert', 'deserve', 'deserved', 'deserves', 'design', 'designed', 'designs', 'desire', 'desires', 'despair', 'desperate', 'desperately', 'desperation', 'despicable', 'despite', 'destiny', 'destroy', 'destroyed', 'destroying', 'destruction', 'detail', 'detailed', 'details', 'detective', 'determined', 'develop', 'developed', 'developing', 'development', 'develops', 'device', 'devil', 'devoid', 'devoted', 'dialog', 'dialogs', 'dialogue', 'dialogues', 'diamond', 'diana', 'diane', 'dick', 'dickens', 'die', 'died', 'dies', 'difference', 'differences', 'different', 'difficult', 'dig', 'digital', 'dignity', 'dimension', 'dimensional', 'din', 'dinner', 'dinosaur', 'dinosaurs', 'dire', 'direct', 'directed', 'directing', 'direction', 'directions', 'directly', 'director', 'directorial', 'directors', 'directs', 'dirty', 'disagree', 'disappear', 'disappeared', 'disappears', 'disappoint', 'disappointed', 'disappointing', 'disappointment', 'disaster', 'disbelief', 'disc', 'discover', 'discovered', 'discovering', 'discovers', 'discovery', 'discuss', 'discussion', 'disease', 'disgusting', 'disjointed', 'dislike', 'disney', 'display', 'displayed', 'displays', 'distance', 'distant', 'distinct', 'distracting', 'distribution', 'disturbed', 'disturbing', 'divorce', 'dixon', 'doc', 'doctor', 'documentaries', 'documentary', 'dog', 'dogs', 'doll', 'dollar', 'dollars', 'dolls', 'dolph', 'domestic', 'domino', 'donald', 'done', 'donna', 'doo', 'doom', 'doomed', 'door', 'doors', 'dorothy', 'dose', 'double', 'doubt', 'douglas', 'downey', 'downhill', 'downright', 'dozen', 'dozens', 'dr', 'dracula', 'drag', 'dragged', 'dragon', 'drags', 'drake', 'drama', 'dramas', 'dramatic', 'draw', 'drawing', 'drawn', 'draws', 'dreadful', 'dream', 'dreams', 'dreary', 'dreck', 'dress', 'dressed', 'dressing', 'drew', 'drink', 'drinking', 'drive', 'drivel', 'driven', 'driver', 'drives', 'driving', 'drop', 'dropped', 'dropping', 'drops', 'drug', 'drugs', 'drunk', 'drunken', 'dry', 'dub', 'dubbed', 'dubbing', 'dud', 'dude', 'due', 'duke', 'dull', 'dumb', 'duo', 'dust', 'dutch', 'duty', 'duvall', 'dvd', 'dying', 'dylan', 'dynamic', 'dysfunctional', 'eager', 'ear', 'earl', 'earlier', 'early', 'earned', 'ears', 'earth', 'ease', 'easier', 'easily', 'east', 'eastern', 'eastwood', 'easy', 'eat', 'eaten', 'eating', 'ebert', 'eccentric', 'ed', 'eddie', 'edgar', 'edge', 'edgy', 'edie', 'edited', 'editing', 'edition', 'editor', 'education', 'edward', 'eerie', 'effect', 'effective', 'effectively', 'effects', 'effort', 'efforts', 'ego', 'eight', 'eighties', 'either', 'el', 'elaborate', 'elderly', 'elegant', 'element', 'elements', 'elephant', 'elizabeth', 'ellen', 'elm', 'else', 'elsewhere', 'elvira', 'elvis', 'em', 'embarrassed', 'embarrassing', 'embarrassment', 'emily', 'emma', 'emotion', 'emotional', 'emotionally', 'emotions', 'empathy', 'emperor', 'emphasis', 'empire', 'employed', 'empty', 'en', 'encounter', 'encounters', 'end', 'endearing', 'ended', 'ending', 'endings', 'endless', 'ends', 'endure', 'enemies', 'enemy', 'energy', 'engage', 'engaged', 'engaging', 'england', 'english', 'enjoy', 'enjoyable', 'enjoyed', 'enjoying', 'enjoyment', 'enjoys', 'enormous', 'enough', 'ensemble', 'ensues', 'enter', 'enterprise', 'enters', 'entertain', 'entertained', 'entertaining', 'entertainment', 'enthusiasm', 'entire', 'entirely', 'entry', 'environment', 'epic', 'episode', 'episodes', 'equal', 'equally', 'equipment', 'er', 'era', 'eric', 'erotic', 'errors', 'escape', 'escaped', 'escapes', 'especially', 'essence', 'essential', 'essentially', 'established', 'estate', 'esther', 'et', 'etc', 'ethan', 'eugene', 'europe', 'european', 'eva', 'eve', 'even', 'evening', 'event', 'events', 'eventually', 'ever', 'every', 'everybody', 'everyday', 'everyone', 'everything', 'everywhere', 'evidence', 'evident', 'evil', 'ex', 'exact', 'exactly', 'exaggerated', 'examination', 'example', 'examples', 'excellent', 'except', 'exception', 'exceptional', 'exceptionally', 'exceptions', 'excited', 'excitement', 'exciting', 'excuse', 'executed', 'execution', 'executive', 'exercise', 'exist', 'existed', 'existence', 'existent', 'exists', 'exotic', 'expect', 'expectations', 'expected', 'expecting', 'expensive', 'experience', 'experienced', 'experiences', 'experiment', 'experimental', 'experiments', 'expert', 'explain', 'explained', 'explaining', 'explains', 'explanation', 'explicit', 'exploitation', 'exploration', 'explore', 'explored', 'explosion', 'explosions', 'exposed', 'exposition', 'exposure', 'express', 'expressed', 'expression', 'expressions', 'extended', 'extent', 'extra', 'extraordinary', 'extras', 'extreme', 'extremely', 'eye', 'eyed', 'eyes', 'eyre', 'fabulous', 'face', 'faced', 'faces', 'facial', 'facing', 'fact', 'factor', 'factory', 'facts', 'fail', 'failed', 'failing', 'fails', 'failure', 'fair', 'fairly', 'fairy', 'faith', 'faithful', 'fake', 'falk', 'fall', 'fallen', 'falling', 'falls', 'false', 'fame', 'familiar', 'families', 'family', 'famous', 'fan', 'fancy', 'fans', 'fantastic', 'fantasy', 'far', 'farce', 'fare', 'farm', 'farrell', 'fascinated', 'fascinating', 'fascination', 'fashion', 'fashioned', 'fast', 'faster', 'fat', 'fatal', 'fate', 'father', 'fault', 'faults', 'favor', 'favorite', 'favorites', 'favourite', 'fay', 'fbi', 'fear', 'fears', 'feature', 'featured', 'features', 'featuring', 'feed', 'feel', 'feeling', 'feelings', 'feels', 'feet', 'felix', 'fell', 'fellow', 'felt', 'female', 'feminist', 'femme', 'fest', 'festival', 'fetched', 'fever', 'fi', 'fianc', 'fiction', 'fictional', 'field', 'fields', 'fifteen', 'fifty', 'fight', 'fighter', 'fighting', 'fights', 'figure', 'figured', 'figures', 'files', 'fill', 'filled', 'filling', 'film', 'filmed', 'filming', 'filmmaker', 'filmmakers', 'films', 'final', 'finale', 'finally', 'financial', 'find', 'finding', 'finds', 'fine', 'finest', 'finger', 'finish', 'finished', 'fire', 'fired', 'first', 'firstly', 'fish', 'fisher', 'fit', 'fits', 'fitting', 'five', 'fix', 'flair', 'flash', 'flashback', 'flashbacks', 'flat', 'flaw', 'flawed', 'flawless', 'flaws', 'flesh', 'flick', 'flicks', 'flies', 'flight', 'floating', 'floor', 'flop', 'florida', 'flow', 'fly', 'flying', 'flynn', 'focus', 'focused', 'focuses', 'focusing', 'folk', 'folks', 'follow', 'followed', 'following', 'follows', 'fond', 'fonda', 'food', 'fool', 'fooled', 'foot', 'footage', 'football', 'forbidden', 'force', 'forced', 'forces', 'ford', 'foreign', 'forest', 'forever', 'forget', 'forgettable', 'forgive', 'forgot', 'forgotten', 'form', 'format', 'former', 'forms', 'formula', 'formulaic', 'forth', 'fortunately', 'fortune', 'forty', 'forward', 'foster', 'foul', 'found', 'four', 'fourth', 'fox', 'foxx', 'frame', 'france', 'franchise', 'francis', 'francisco', 'franco', 'frank', 'frankenstein', 'frankie', 'frankly', 'freak', 'freaks', 'fred', 'freddy', 'free', 'freedom', 'freeman', 'french', 'frequent', 'frequently', 'fresh', 'friday', 'friend', 'friendly', 'friends', 'friendship', 'frightening', 'front', 'frustrated', 'frustration', 'fu', 'fulci', 'full', 'fuller', 'fully', 'fun', 'funeral', 'funnier', 'funniest', 'funny', 'furious', 'furthermore', 'fury', 'future', 'futuristic', 'fx', 'gabriel', 'gadget', 'gag', 'gags', 'gain', 'gambling', 'game', 'games', 'gandhi', 'gang', 'gangster', 'gangsters', 'garbage', 'garbo', 'garden', 'garner', 'gary', 'gas', 'gather', 'gave', 'gay', 'gem', 'gender', 'gene', 'general', 'generally', 'generation', 'generic', 'generous', 'genius', 'genre', 'genres', 'gentle', 'gentleman', 'genuine', 'genuinely', 'george', 'gerard', 'german', 'germans', 'germany', 'get', 'gets', 'getting', 'ghost', 'ghosts', 'giallo', 'giant', 'gift', 'gifted', 'gillian', 'gina', 'ginger', 'girl', 'girlfriend', 'girls', 'give', 'given', 'gives', 'giving', 'glad', 'glass', 'glasses', 'glenn', 'glimpse', 'global', 'glorious', 'glory', 'glover', 'go', 'goal', 'god', 'godfather', 'godzilla', 'goes', 'going', 'gold', 'goldberg', 'golden', 'gone', 'gonna', 'good', 'goodness', 'goofy', 'gordon', 'gore', 'gorgeous', 'gory', 'got', 'gothic', 'gotta', 'gotten', 'government', 'grab', 'grace', 'grade', 'gradually', 'graham', 'grand', 'grandfather', 'grandmother', 'grant', 'granted', 'graphic', 'graphics', 'grasp', 'gratuitous', 'grave', 'gray', 'grayson', 'great', 'greater', 'greatest', 'greatly', 'greatness', 'greedy', 'greek', 'green', 'greg', 'gregory', 'grew', 'grey', 'griffith', 'grim', 'grinch', 'gripping', 'gritty', 'gross', 'ground', 'group', 'groups', 'grow', 'growing', 'grown', 'grows', 'gruesome', 'guarantee', 'guaranteed', 'guard', 'guess', 'guessed', 'guessing', 'guest', 'guide', 'guilt', 'guilty', 'gun', 'guns', 'guts', 'guy', 'guys', 'ha', 'hair', 'hal', 'half', 'halfway', 'hall', 'halloween', 'ham', 'hamilton', 'hamlet', 'hammer', 'hand', 'handed', 'handful', 'handle', 'handled', 'hands', 'handsome', 'hang', 'hanging', 'hank', 'hanks', 'happen', 'happened', 'happening', 'happens', 'happily', 'happiness', 'happy', 'hard', 'hardcore', 'harder', 'hardly', 'hardy', 'harm', 'harris', 'harry', 'harsh', 'hart', 'harvey', 'hat', 'hate', 'hated', 'hates', 'hatred', 'haunted', 'haunting', 'hawke', 'hbo', 'head', 'headed', 'heads', 'health', 'hear', 'heard', 'hearing', 'heart', 'hearted', 'hearts', 'heat', 'heaven', 'heavily', 'heavy', 'heck', 'heist', 'held', 'helen', 'helicopter', 'hell', 'hello', 'help', 'helped', 'helping', 'helps', 'hence', 'henry', 'hero', 'heroes', 'heroic', 'heroine', 'heston', 'hey', 'hidden', 'hide', 'hideous', 'hiding', 'high', 'higher', 'highest', 'highlight', 'highlights', 'highly', 'hilarious', 'hilariously', 'hill', 'hills', 'hint', 'hints', 'hip', 'hippie', 'hire', 'hired', 'historical', 'historically', 'history', 'hit', 'hitchcock', 'hitler', 'hits', 'hitting', 'ho', 'hoffman', 'hold', 'holding', 'holds', 'hole', 'holes', 'holiday', 'hollow', 'holly', 'hollywood', 'holmes', 'holocaust', 'holy', 'homage', 'home', 'homeless', 'homosexual', 'honest', 'honestly', 'honesty', 'hong', 'honor', 'hood', 'hook', 'hooked', 'hop', 'hope', 'hoped', 'hopefully', 'hopeless', 'hopes', 'hoping', 'hopkins', 'hopper', 'horrendous', 'horrible', 'horribly', 'horrid', 'horrific', 'horrifying', 'horror', 'horrors', 'horse', 'horses', 'hospital', 'host', 'hot', 'hotel', 'hour', 'hours', 'house', 'household', 'houses', 'howard', 'however', 'hudson', 'huge', 'hugh', 'huh', 'human', 'humanity', 'humans', 'humble', 'humor', 'humorous', 'humour', 'hundred', 'hundreds', 'hung', 'hungry', 'hunt', 'hunter', 'hunters', 'hunting', 'hurt', 'hurts', 'husband', 'husbands', 'hyde', 'hype', 'hysterical', 'ian', 'ice', 'idea', 'ideal', 'ideas', 'identify', 'identity', 'idiot', 'idiotic', 'idiots', 'ignorant', 'ignore', 'ignored', 'ii', 'iii', 'ill', 'illegal', 'illness', 'illogical', 'image', 'imagery', 'images', 'imagination', 'imaginative', 'imagine', 'imagined', 'imdb', 'imitation', 'immediately', 'immensely', 'impact', 'implausible', 'importance', 'important', 'importantly', 'impossible', 'impress', 'impressed', 'impression', 'impressive', 'improve', 'improved', 'improvement', 'inability', 'inane', 'inappropriate', 'incident', 'include', 'included', 'includes', 'including', 'incoherent', 'incompetent', 'incomprehensible', 'increasingly', 'incredible', 'incredibly', 'indeed', 'independence', 'independent', 'india', 'indian', 'indians', 'indication', 'indie', 'individual', 'individuals', 'inducing', 'industry', 'inept', 'inevitable', 'infamous', 'infected', 'inferior', 'influence', 'influenced', 'information', 'ingredients', 'initial', 'initially', 'inner', 'innocence', 'innocent', 'innovative', 'insane', 'inside', 'insight', 'insists', 'inspector', 'inspiration', 'inspired', 'inspiring', 'installment', 'instance', 'instant', 'instantly', 'instead', 'insult', 'insulting', 'insurance', 'intellectual', 'intelligence', 'intelligent', 'intended', 'intense', 'intensity', 'intent', 'intention', 'intentionally', 'intentions', 'interaction', 'interactions', 'interest', 'interested', 'interesting', 'international', 'internet', 'interpretation', 'interview', 'interviews', 'intimate', 'intrigue', 'intrigued', 'intriguing', 'introduce', 'introduced', 'introduces', 'introduction', 'invasion', 'inventive', 'investigate', 'investigating', 'investigation', 'invisible', 'involve', 'involved', 'involvement', 'involves', 'involving', 'iran', 'iraq', 'ireland', 'irish', 'ironic', 'ironically', 'irony', 'irritating', 'island', 'isolated', 'israel', 'issue', 'issues', 'italian', 'italy', 'items', 'jack', 'jackie', 'jackson', 'jail', 'jake', 'james', 'jamie', 'jane', 'japan', 'japanese', 'jason', 'jaw', 'jaws', 'jay', 'jazz', 'jealous', 'jean', 'jeff', 'jeffrey', 'jennifer', 'jenny', 'jeremy', 'jerk', 'jerry', 'jesse', 'jessica', 'jesus', 'jet', 'jewish', 'jim', 'jimmy', 'joan', 'job', 'jobs', 'joe', 'joel', 'joey', 'john', 'johnny', 'johnson', 'join', 'joined', 'joins', 'joke', 'jokes', 'jon', 'jonathan', 'jones', 'joseph', 'josh', 'journalist', 'journey', 'joy', 'jr', 'judge', 'judging', 'judy', 'julia', 'julian', 'julie', 'jump', 'jumped', 'jumping', 'jumps', 'june', 'jungle', 'junior', 'junk', 'justice', 'justify', 'justin', 'juvenile', 'kane', 'kapoor', 'karen', 'karloff', 'kate', 'kay', 'keaton', 'keep', 'keeping', 'keeps', 'keith', 'kelly', 'ken', 'kennedy', 'kenneth', 'kept', 'kevin', 'key', 'khan', 'kick', 'kicked', 'kicking', 'kicks', 'kid', 'kidding', 'kidnapped', 'kids', 'kill', 'killed', 'killer', 'killers', 'killing', 'killings', 'kills', 'kim', 'kind', 'kinda', 'kinds', 'king', 'kingdom', 'kirk', 'kiss', 'kissing', 'kitchen', 'kline', 'knew', 'knife', 'knock', 'knocked', 'know', 'knowing', 'knowledge', 'known', 'knows', 'kolchak', 'kong', 'korean', 'kubrick', 'kudos', 'kung', 'kurt', 'kyle', 'la', 'lab', 'lack', 'lacked', 'lacking', 'lackluster', 'lacks', 'ladies', 'lady', 'laid', 'lake', 'lame', 'lampoon', 'land', 'landscape', 'landscapes', 'lane', 'language', 'large', 'largely', 'larger', 'larry', 'last', 'lasted', 'late', 'lately', 'later', 'latest', 'latin', 'latter', 'laugh', 'laughable', 'laughed', 'laughing', 'laughs', 'laughter', 'laura', 'laurel', 'laurence', 'law', 'lawrence', 'laws', 'lawyer', 'lay', 'lazy', 'le', 'lead', 'leader', 'leading', 'leads', 'league', 'learn', 'learned', 'learning', 'learns', 'least', 'leave', 'leaves', 'leaving', 'led', 'lee', 'left', 'leg', 'legal', 'legend', 'legendary', 'legs', 'lemmon', 'lena', 'length', 'lengthy', 'leo', 'leon', 'leonard', 'lesbian', 'leslie', 'less', 'lesser', 'lesson', 'lessons', 'let', 'lets', 'letter', 'letters', 'letting', 'level', 'levels', 'lewis', 'lex', 'li', 'liberal', 'library', 'lie', 'lies', 'life', 'lifestyle', 'lifetime', 'light', 'lighting', 'lights', 'likable', 'like', 'liked', 'likely', 'likes', 'likewise', 'liking', 'lily', 'limited', 'limits', 'lincoln', 'linda', 'line', 'liners', 'lines', 'link', 'lion', 'lips', 'lisa', 'list', 'listed', 'listen', 'listening', 'lit', 'literally', 'literature', 'little', 'live', 'lived', 'lively', 'lives', 'living', 'lloyd', 'load', 'loaded', 'loads', 'local', 'location', 'locations', 'locked', 'logic', 'logical', 'lol', 'london', 'lone', 'lonely', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'loose', 'loosely', 'lord', 'los', 'lose', 'loser', 'losers', 'loses', 'losing', 'loss', 'lost', 'lot', 'lots', 'lou', 'loud', 'louis', 'louise', 'lousy', 'lovable', 'love', 'loved', 'lovely', 'lover', 'lovers', 'loves', 'loving', 'low', 'lower', 'lowest', 'loyal', 'loyalty', 'lucas', 'luck', 'luckily', 'lucky', 'lucy', 'ludicrous', 'lugosi', 'luis', 'lukas', 'luke', 'lumet', 'lundgren', 'lush', 'lust', 'lying', 'lynch', 'lyrics', 'macarthur', 'machine', 'machines', 'macy', 'mad', 'made', 'madness', 'madonna', 'mafia', 'magazine', 'maggie', 'magic', 'magical', 'magnificent', 'maid', 'mail', 'main', 'mainly', 'mainstream', 'maintain', 'major', 'majority', 'make', 'maker', 'makers', 'makes', 'makeup', 'making', 'male', 'mall', 'man', 'manage', 'managed', 'manager', 'manages', 'manhattan', 'manipulative', 'mankind', 'mann', 'manner', 'mansion', 'many', 'map', 'march', 'margaret', 'maria', 'marie', 'mario', 'marion', 'mark', 'market', 'marks', 'marriage', 'married', 'marry', 'mars', 'marshall', 'martial', 'martin', 'marvelous', 'mary', 'mask', 'masks', 'mass', 'massacre', 'masses', 'massive', 'master', 'masterful', 'masterpiece', 'masterpieces', 'masters', 'match', 'matched', 'matches', 'mate', 'material', 'matrix', 'matt', 'matter', 'matters', 'matthau', 'matthew', 'mature', 'max', 'may', 'maybe', 'mayor', 'mccoy', 'mean', 'meaning', 'meaningful', 'meaningless', 'means', 'meant', 'meanwhile', 'measure', 'meat', 'mechanical', 'media', 'medical', 'mediocre', 'medium', 'meet', 'meeting', 'meets', 'meg', 'mel', 'melodrama', 'melodramatic', 'melting', 'member', 'members', 'memorable', 'memories', 'memory', 'men', 'menace', 'menacing', 'mental', 'mentally', 'mention', 'mentioned', 'mentions', 'mere', 'merely', 'merit', 'mess', 'message', 'messages', 'messed', 'met', 'metal', 'metaphor', 'method', 'mexican', 'mexico', 'mgm', 'michael', 'michelle', 'mickey', 'mid', 'middle', 'midnight', 'might', 'mighty', 'miike', 'mike', 'mild', 'mildly', 'mildred', 'mile', 'miles', 'military', 'milk', 'mill', 'miller', 'million', 'millionaire', 'millions', 'min', 'mind', 'minded', 'mindless', 'minds', 'mine', 'mini', 'minimal', 'minimum', 'minister', 'minor', 'minute', 'minutes', 'miracle', 'mirror', 'miscast', 'miserable', 'miserably', 'misery', 'miss', 'missed', 'misses', 'missile', 'missing', 'mission', 'mistake', 'mistaken', 'mistakes', 'mistress', 'mitchell', 'mix', 'mixed', 'mixture', 'mob', 'model', 'models', 'modern', 'modesty', 'molly', 'mom', 'moment', 'moments', 'mon', 'money', 'monkey', 'monkeys', 'monster', 'monsters', 'montage', 'montana', 'month', 'months', 'mood', 'moody', 'moon', 'moore', 'moral', 'morality', 'morgan', 'morning', 'moronic', 'mostly', 'mother', 'motion', 'motivation', 'motivations', 'motives', 'mountain', 'mountains', 'mouse', 'mouth', 'move', 'moved', 'movement', 'movements', 'moves', 'movie', 'movies', 'moving', 'mr', 'mrs', 'ms', 'mst', 'much', 'multi', 'multiple', 'mummy', 'mundane', 'muppet', 'murder', 'murdered', 'murderer', 'murdering', 'murderous', 'murders', 'murphy', 'museum', 'music', 'musical', 'musicals', 'musician', 'muslim', 'must', 'myers', 'mysteries', 'mysterious', 'mystery', 'nail', 'naive', 'naked', 'name', 'named', 'namely', 'names', 'nancy', 'narration', 'narrative', 'narrator', 'nasty', 'nathan', 'nation', 'national', 'native', 'natural', 'naturally', 'nature', 'navy', 'nazi', 'nazis', 'nd', 'near', 'nearby', 'nearly', 'neat', 'necessarily', 'necessary', 'neck', 'ned', 'need', 'needed', 'needless', 'needs', 'negative', 'neighbor', 'neighborhood', 'neighbors', 'neil', 'neither', 'nelson', 'nemesis', 'neo', 'nerd', 'nervous', 'network', 'never', 'nevertheless', 'new', 'newly', 'newman', 'news', 'newspaper', 'next', 'nice', 'nicely', 'nicholas', 'nicholson', 'nick', 'nicole', 'night', 'nightmare', 'nightmares', 'nights', 'nine', 'ninja', 'niro', 'noble', 'nobody', 'noir', 'noise', 'noises', 'nominated', 'nomination', 'non', 'none', 'nonetheless', 'nonsense', 'nonsensical', 'normal', 'normally', 'norman', 'north', 'nose', 'nostalgia', 'nostalgic', 'notable', 'notably', 'notch', 'note', 'noted', 'notes', 'nothing', 'notice', 'noticed', 'notion', 'notorious', 'novak', 'novel', 'novels', 'nowadays', 'nowhere', 'nuclear', 'nude', 'nudity', 'number', 'numbers', 'numerous', 'nurse', 'nuts', 'nyc', 'object', 'obnoxious', 'obscure', 'obsessed', 'obsession', 'obvious', 'obviously', 'occasion', 'occasional', 'occasionally', 'occasions', 'occur', 'occurred', 'occurs', 'ocean', 'odd', 'oddly', 'odds', 'offended', 'offensive', 'offer', 'offered', 'offering', 'offers', 'office', 'officer', 'officers', 'official', 'often', 'oh', 'oil', 'ok', 'okay', 'old', 'older', 'oliver', 'olivier', 'ollie', 'one', 'ones', 'onto', 'open', 'opened', 'opening', 'opens', 'opera', 'operation', 'opinion', 'opinions', 'opportunity', 'opposed', 'opposite', 'orange', 'order', 'orders', 'ordinary', 'original', 'originality', 'originally', 'orson', 'oscar', 'oscars', 'others', 'otherwise', 'ought', 'outcome', 'outer', 'outfit', 'outing', 'outrageous', 'outside', 'outstanding', 'overacting', 'overall', 'overcome', 'overdone', 'overlook', 'overlooked', 'overly', 'overrated', 'overwhelming', 'owner', 'oz', 'pace', 'paced', 'pacing', 'pacino', 'pack', 'package', 'packed', 'page', 'paid', 'pain', 'painful', 'painfully', 'paint', 'painted', 'painting', 'pair', 'pal', 'palace', 'palance', 'palma', 'paltrow', 'pan', 'panic', 'pants', 'paper', 'par', 'parade', 'paradise', 'parallel', 'paranoia', 'parent', 'parents', 'paris', 'park', 'parker', 'parody', 'part', 'particular', 'particularly', 'parties', 'partly', 'partner', 'parts', 'party', 'pass', 'passable', 'passed', 'passes', 'passing', 'passion', 'passionate', 'past', 'pat', 'path', 'pathetic', 'patience', 'patient', 'patients', 'patrick', 'paul', 'paulie', 'pay', 'paying', 'pays', 'peace', 'peak', 'pearl', 'penn', 'penny', 'people', 'peoples', 'per', 'perfect', 'perfection', 'perfectly', 'perform', 'performance', 'performances', 'performed', 'performer', 'performers', 'performing', 'perhaps', 'period', 'perry', 'person', 'persona', 'personal', 'personalities', 'personality', 'personally', 'persons', 'perspective', 'pet', 'pete', 'peter', 'peters', 'pg', 'phantom', 'philip', 'philosophical', 'philosophy', 'phone', 'phony', 'photo', 'photographed', 'photographer', 'photography', 'photos', 'physical', 'physically', 'piano', 'pick', 'picked', 'picking', 'picks', 'picture', 'pictures', 'pie', 'piece', 'pieces', 'pierce', 'pig', 'pile', 'pilot', 'pin', 'pink', 'pit', 'pitch', 'pitiful', 'pitt', 'pity', 'place', 'placed', 'places', 'plague', 'plain', 'plan', 'plane', 'planet', 'planned', 'planning', 'plans', 'plant', 'plastic', 'plausible', 'play', 'played', 'player', 'players', 'playing', 'plays', 'pleasant', 'pleasantly', 'please', 'pleased', 'pleasure', 'plenty', 'plight', 'plot', 'plots', 'plus', 'poetic', 'poetry', 'poignant', 'point', 'pointed', 'pointless', 'points', 'poison', 'pokemon', 'polanski', 'police', 'policeman', 'polished', 'political', 'politically', 'politics', 'pool', 'poor', 'poorly', 'pop', 'popcorn', 'pops', 'popular', 'population', 'porn', 'porno', 'portion', 'portrait', 'portray', 'portrayal', 'portrayed', 'portraying', 'portrays', 'position', 'positive', 'possessed', 'possibilities', 'possibility', 'possible', 'possibly', 'post', 'poster', 'posters', 'pot', 'potential', 'potentially', 'poverty', 'powell', 'power', 'powerful', 'powers', 'practically', 'practice', 'praise', 'pre', 'precious', 'predictable', 'prefer', 'pregnant', 'preminger', 'premise', 'prepared', 'prequel', 'presence', 'present', 'presentation', 'presented', 'presenting', 'presents', 'president', 'press', 'pressure', 'presumably', 'pretend', 'pretending', 'pretentious', 'pretty', 'prevent', 'preview', 'previous', 'previously', 'prey', 'price', 'priceless', 'pride', 'priest', 'primarily', 'primary', 'prime', 'prince', 'princess', 'principal', 'print', 'prior', 'prison', 'prisoner', 'prisoners', 'private', 'prize', 'pro', 'probably', 'problem', 'problems', 'proceedings', 'proceeds', 'process', 'produce', 'produced', 'producer', 'producers', 'producing', 'product', 'production', 'productions', 'professional', 'professor', 'profound', 'program', 'progress', 'progresses', 'project', 'projects', 'prom', 'prominent', 'promise', 'promised', 'promises', 'promising', 'promote', 'proof', 'propaganda', 'proper', 'properly', 'property', 'props', 'prostitute', 'protagonist', 'protagonists', 'protect', 'proud', 'prove', 'proved', 'proves', 'provide', 'provided', 'provides', 'providing', 'provoking', 'pseudo', 'psychiatrist', 'psycho', 'psychological', 'psychotic', 'public', 'pull', 'pulled', 'pulling', 'pulls', 'pulp', 'punch', 'punishment', 'punk', 'purchase', 'purchased', 'pure', 'purely', 'purple', 'purpose', 'purposes', 'pursuit', 'push', 'pushed', 'pushing', 'put', 'puts', 'putting', 'quaid', 'qualities', 'quality', 'queen', 'quest', 'question', 'questionable', 'questions', 'quick', 'quickly', 'quiet', 'quinn', 'quirky', 'quit', 'quite', 'quote', 'quotes', 'rabbit', 'race', 'rachel', 'racial', 'racism', 'racist', 'radio', 'rage', 'rain', 'raise', 'raised', 'raising', 'ralph', 'rambo', 'ramones', 'ran', 'random', 'randomly', 'randy', 'range', 'rangers', 'rank', 'ranks', 'rap', 'rape', 'raped', 'rare', 'rarely', 'rat', 'rate', 'rated', 'rates', 'rather', 'rating', 'ratings', 'rats', 'ratso', 'rave', 'raw', 'ray', 'raymond', 'rd', 'reach', 'reached', 'reaches', 'reaching', 'react', 'reaction', 'reactions', 'read', 'reader', 'reading', 'reads', 'ready', 'real', 'realise', 'realism', 'realistic', 'reality', 'realize', 'realized', 'realizes', 'realizing', 'really', 'reason', 'reasonable', 'reasonably', 'reasons', 'rebel', 'recall', 'receive', 'received', 'receives', 'recent', 'recently', 'recognition', 'recognize', 'recognized', 'recommend', 'recommended', 'record', 'recorded', 'recording', 'red', 'redeeming', 'redemption', 'reduced', 'reed', 'reel', 'reference', 'references', 'reflect', 'reflection', 'refreshing', 'refuse', 'refused', 'refuses', 'regard', 'regarded', 'regarding', 'regardless', 'regret', 'regular', 'relate', 'related', 'relations', 'relationship', 'relationships', 'relative', 'relatively', 'relatives', 'relax', 'release', 'released', 'relevant', 'relief', 'relies', 'religion', 'religious', 'remain', 'remaining', 'remains', 'remake', 'remakes', 'remarkable', 'remarkably', 'remarks', 'remember', 'remembered', 'remind', 'reminded', 'reminds', 'reminiscent', 'remote', 'remotely', 'removed', 'rendition', 'rent', 'rental', 'rented', 'renting', 'repeat', 'repeated', 'repeatedly', 'repetitive', 'replace', 'replaced', 'report', 'reporter', 'represent', 'represented', 'represents', 'reputation', 'required', 'requires', 'rescue', 'research', 'resemblance', 'resembles', 'resident', 'resolution', 'resort', 'resources', 'respect', 'respected', 'response', 'responsibility', 'responsible', 'rest', 'restaurant', 'restored', 'result', 'resulting', 'results', 'retarded', 'retired', 'return', 'returned', 'returning', 'returns', 'reunion', 'reveal', 'revealed', 'revealing', 'reveals', 'revelation', 'revenge', 'review', 'reviewer', 'reviewers', 'reviews', 'revolution', 'revolutionary', 'revolves', 'rex', 'reynolds', 'rich', 'richard', 'rick', 'rid', 'ridden', 'ride', 'ridiculous', 'ridiculously', 'riding', 'right', 'rights', 'ring', 'rings', 'riot', 'rip', 'ripped', 'rise', 'rises', 'rising', 'risk', 'rita', 'ritter', 'rival', 'river', 'riveting', 'road', 'rob', 'robbery', 'robert', 'roberts', 'robin', 'robinson', 'robot', 'robots', 'rochester', 'rock', 'rocket', 'rocks', 'rocky', 'roger', 'rogers', 'role', 'roles', 'roll', 'rolling', 'roman', 'romance', 'romantic', 'romero', 'ron', 'roof', 'room', 'roommate', 'rooms', 'rooney', 'root', 'roots', 'rose', 'roth', 'rotten', 'rough', 'round', 'routine', 'row', 'roy', 'rubber', 'rubbish', 'ruby', 'ruin', 'ruined', 'ruins', 'rukh', 'rule', 'rules', 'run', 'running', 'runs', 'rural', 'rush', 'rushed', 'russell', 'russia', 'russian', 'ruth', 'ruthless', 'ryan', 'sabrina', 'sacrifice', 'sad', 'sadistic', 'sadly', 'sadness', 'safe', 'safety', 'saga', 'said', 'sake', 'sally', 'sam', 'samurai', 'san', 'sandler', 'sandra', 'santa', 'sappy', 'sarah', 'sat', 'satan', 'satire', 'satisfied', 'satisfy', 'satisfying', 'saturday', 'savage', 'save', 'saved', 'saves', 'saving', 'saw', 'say', 'saying', 'says', 'scale', 'scare', 'scarecrow', 'scared', 'scares', 'scary', 'scenario', 'scene', 'scenery', 'scenes', 'scheme', 'school', 'schools', 'sci', 'science', 'scientific', 'scientist', 'scientists', 'scooby', 'scope', 'score', 'scores', 'scott', 'scottish', 'scream', 'screaming', 'screams', 'screen', 'screening', 'screenplay', 'screens', 'screenwriter', 'screwed', 'script', 'scripted', 'scripts', 'scrooge', 'sea', 'seagal', 'sean', 'search', 'searching', 'season', 'seasons', 'seat', 'second', 'secondary', 'secondly', 'seconds', 'secret', 'secretary', 'secretly', 'secrets', 'section', 'security', 'see', 'seed', 'seeing', 'seek', 'seeking', 'seeks', 'seem', 'seemed', 'seemingly', 'seems', 'seen', 'sees', 'segment', 'segments', 'self', 'selfish', 'sell', 'selling', 'semi', 'send', 'sends', 'sense', 'senseless', 'sensitive', 'sent', 'sentence', 'sentimental', 'sentinel', 'separate', 'september', 'sequel', 'sequels', 'sequence', 'sequences', 'serial', 'series', 'serious', 'seriously', 'serve', 'served', 'serves', 'service', 'serving', 'set', 'sets', 'setting', 'settings', 'settle', 'seven', 'seventies', 'several', 'severe', 'sex', 'sexual', 'sexuality', 'sexually', 'sexy', 'sh', 'shadow', 'shadows', 'shake', 'shakespeare', 'shall', 'shallow', 'shame', 'shanghai', 'shape', 'share', 'shared', 'shark', 'sharon', 'sharp', 'shaw', 'shed', 'sheer', 'shelf', 'shelley', 'sheriff', 'shine', 'shines', 'shining', 'ship', 'shirley', 'shirt', 'shock', 'shocked', 'shocking', 'shoddy', 'shoes', 'shoot', 'shooting', 'shoots', 'shop', 'short', 'shortly', 'shorts', 'shot', 'shots', 'shoulder', 'shoulders', 'show', 'showcase', 'showdown', 'showed', 'shower', 'showing', 'shown', 'shows', 'shut', 'shy', 'sick', 'sid', 'side', 'sidekick', 'sides', 'sidney', 'sight', 'sign', 'signed', 'significance', 'significant', 'signs', 'silence', 'silent', 'silliness', 'silly', 'silver', 'similar', 'similarities', 'similarly', 'simmons', 'simon', 'simple', 'simplicity', 'simplistic', 'simply', 'simpson', 'simultaneously', 'sin', 'sinatra', 'since', 'sincere', 'sing', 'singer', 'singers', 'singing', 'single', 'sings', 'sinister', 'sink', 'sir', 'sissy', 'sister', 'sisters', 'sit', 'sitcom', 'site', 'sits', 'sitting', 'situation', 'situations', 'six', 'sixties', 'size', 'sketch', 'skill', 'skills', 'skin', 'skip', 'skits', 'sky', 'slap', 'slapstick', 'slasher', 'slaughter', 'slave', 'sleazy', 'sleep', 'sleeping', 'slice', 'slick', 'slight', 'slightest', 'slightly', 'sloppy', 'slow', 'slowly', 'small', 'smaller', 'smart', 'smile', 'smiling', 'smith', 'smoke', 'smoking', 'smooth', 'snake', 'sneak', 'snl', 'snow', 'soap', 'soccer', 'social', 'society', 'soderbergh', 'soft', 'sold', 'soldier', 'soldiers', 'sole', 'solely', 'solid', 'solo', 'solution', 'solve', 'somebody', 'somehow', 'someone', 'something', 'sometimes', 'somewhat', 'somewhere', 'son', 'song', 'songs', 'sons', 'soon', 'sophisticated', 'sopranos', 'sorry', 'sort', 'sorts', 'soul', 'souls', 'sound', 'sounded', 'sounding', 'sounds', 'soundtrack', 'source', 'south', 'southern', 'soviet', 'space', 'spacey', 'spain', 'spanish', 'spare', 'speak', 'speaking', 'speaks', 'special', 'specially', 'species', 'specific', 'specifically', 'spectacular', 'speech', 'speed', 'spell', 'spend', 'spending', 'spends', 'spent', 'spider', 'spielberg', 'spike', 'spin', 'spirit', 'spirited', 'spirits', 'spiritual', 'spite', 'splatter', 'splendid', 'split', 'spoil', 'spoiled', 'spoiler', 'spoilers', 'spoke', 'spoken', 'spoof', 'spooky', 'sport', 'sports', 'spot', 'spots', 'spread', 'spring', 'spy', 'squad', 'square', 'st', 'staff', 'stage', 'staged', 'stale', 'stan', 'stand', 'standard', 'standards', 'standing', 'stands', 'stanley', 'stanwyck', 'star', 'stargate', 'staring', 'starred', 'starring', 'stars', 'start', 'started', 'starting', 'starts', 'state', 'stated', 'statement', 'states', 'station', 'status', 'stay', 'stayed', 'staying', 'stays', 'steal', 'stealing', 'steals', 'steel', 'stellar', 'step', 'stephen', 'steps', 'stereotype', 'stereotypes', 'stereotypical', 'steve', 'steven', 'stevens', 'stewart', 'stick', 'sticks', 'stiff', 'still', 'stiller', 'stilted', 'stinker', 'stinks', 'stock', 'stole', 'stolen', 'stomach', 'stone', 'stood', 'stooges', 'stop', 'stopped', 'stops', 'store', 'stories', 'storm', 'story', 'storyline', 'storytelling', 'straight', 'strange', 'strangely', 'stranger', 'streep', 'street', 'streets', 'streisand', 'strength', 'stress', 'stretch', 'stretched', 'strictly', 'strike', 'strikes', 'striking', 'string', 'strip', 'strong', 'stronger', 'strongest', 'strongly', 'struck', 'structure', 'struggle', 'struggles', 'struggling', 'stuart', 'stuck', 'student', 'students', 'studio', 'studios', 'study', 'studying', 'stuff', 'stunned', 'stunning', 'stunt', 'stunts', 'stupid', 'stupidity', 'style', 'styles', 'stylish', 'sub', 'subject', 'subjected', 'subjects', 'subplot', 'subplots', 'subsequent', 'substance', 'subtitles', 'subtle', 'subtlety', 'succeed', 'succeeded', 'succeeds', 'success', 'successful', 'successfully', 'suck', 'sucked', 'sucks', 'sudden', 'suddenly', 'sue', 'suffer', 'suffered', 'suffering', 'suffers', 'suffice', 'suggest', 'suggests', 'suicide', 'suit', 'suitable', 'suitably', 'suited', 'suits', 'sullivan', 'sum', 'summary', 'summer', 'sun', 'sunday', 'sung', 'sunny', 'sunshine', 'super', 'superb', 'superbly', 'superficial', 'superhero', 'superior', 'superman', 'supernatural', 'support', 'supporting', 'suppose', 'supposed', 'supposedly', 'sure', 'surely', 'surface', 'surfing', 'surprise', 'surprised', 'surprises', 'surprising', 'surprisingly', 'surreal', 'surrounded', 'surrounding', 'survival', 'survive', 'survived', 'surviving', 'survivor', 'survivors', 'susan', 'suspect', 'suspects', 'suspend', 'suspense', 'suspenseful', 'suspension', 'suspicious', 'sutherland', 'swear', 'swedish', 'sweet', 'swimming', 'switch', 'sword', 'symbolism', 'sympathetic', 'sympathize', 'sympathy', 'synopsis', 'system', 'table', 'tacky', 'tad', 'tag', 'tail', 'take', 'taken', 'takes', 'taking', 'tale', 'talent', 'talented', 'talents', 'tales', 'talk', 'talked', 'talking', 'talks', 'tall', 'tame', 'tank', 'tap', 'tape', 'tarantino', 'target', 'tarzan', 'task', 'taste', 'tasteless', 'tastes', 'taught', 'taxi', 'taylor', 'tea', 'teach', 'teacher', 'team', 'tear', 'tears', 'technical', 'technically', 'technique', 'techniques', 'technology', 'ted', 'tedious', 'teen', 'teenage', 'teenager', 'teenagers', 'teens', 'teeth', 'television', 'tell', 'telling', 'tells', 'temple', 'ten', 'tend', 'tender', 'tends', 'tense', 'tension', 'term', 'terms', 'terrible', 'terribly', 'terrific', 'terrifying', 'territory', 'terror', 'terrorist', 'terrorists', 'terry', 'test', 'texas', 'text', 'th', 'thank', 'thankfully', 'thanks', 'thats', 'theater', 'theaters', 'theatre', 'theatrical', 'theme', 'themes', 'theory', 'therefore', 'thick', 'thief', 'thin', 'thing', 'things', 'think', 'thinking', 'thinks', 'third', 'thirty', 'thomas', 'thompson', 'thoroughly', 'though', 'thought', 'thoughtful', 'thoughts', 'thousand', 'thousands', 'thread', 'threat', 'threatening', 'threatens', 'three', 'threw', 'thrill', 'thriller', 'thrillers', 'thrilling', 'thrills', 'throat', 'throughout', 'throw', 'throwing', 'thrown', 'throws', 'thru', 'thugs', 'thumbs', 'thus', 'ticket', 'tie', 'tied', 'tierney', 'ties', 'tiger', 'tight', 'till', 'tim', 'timberlake', 'time', 'timeless', 'times', 'timing', 'timon', 'timothy', 'tiny', 'tip', 'tired', 'tiresome', 'titanic', 'title', 'titled', 'titles', 'today', 'todd', 'together', 'toilet', 'told', 'tom', 'tomatoes', 'tommy', 'tone', 'tongue', 'tons', 'tony', 'took', 'tooth', 'top', 'topic', 'topless', 'torn', 'torture', 'tortured', 'total', 'totally', 'touch', 'touched', 'touches', 'touching', 'tough', 'tour', 'toward', 'towards', 'town', 'toy', 'track', 'tracks', 'tracy', 'trade', 'tradition', 'traditional', 'tragedy', 'tragic', 'trailer', 'trailers', 'train', 'trained', 'training', 'transfer', 'transformation', 'transition', 'translated', 'translation', 'trap', 'trapped', 'trash', 'trashy', 'travel', 'traveling', 'travels', 'travesty', 'treasure', 'treat', 'treated', 'treatment', 'treats', 'tree', 'trees', 'trek', 'tremendous', 'trial', 'triangle', 'tribe', 'tribute', 'trick', 'tricks', 'tried', 'tries', 'trilogy', 'trio', 'trip', 'trite', 'triumph', 'troops', 'trouble', 'troubled', 'troubles', 'truck', 'true', 'truly', 'trust', 'truth', 'try', 'trying', 'tube', 'tune', 'tunes', 'turkey', 'turn', 'turned', 'turner', 'turning', 'turns', 'tv', 'twelve', 'twenty', 'twice', 'twilight', 'twin', 'twist', 'twisted', 'twists', 'two', 'tyler', 'type', 'types', 'typical', 'typically', 'ugly', 'uk', 'ultimate', 'ultimately', 'ultra', 'un', 'unable', 'unaware', 'unbearable', 'unbelievable', 'unbelievably', 'uncle', 'uncomfortable', 'unconvincing', 'uncut', 'underground', 'underlying', 'underrated', 'understand', 'understandable', 'understanding', 'understated', 'understood', 'underworld', 'undoubtedly', 'uneven', 'unexpected', 'unfolds', 'unforgettable', 'unfortunate', 'unfortunately', 'unfunny', 'unhappy', 'uninspired', 'unintentional', 'unintentionally', 'uninteresting', 'union', 'unique', 'unit', 'united', 'universal', 'universe', 'university', 'unknown', 'unless', 'unlike', 'unlikely', 'unnecessary', 'unoriginal', 'unpleasant', 'unrealistic', 'unseen', 'unsettling', 'unusual', 'unwatchable', 'uplifting', 'upon', 'upper', 'ups', 'upset', 'urban', 'urge', 'us', 'usa', 'use', 'used', 'useful', 'useless', 'user', 'uses', 'using', 'ustinov', 'usual', 'usually', 'utter', 'utterly', 'uwe', 'vacation', 'vader', 'vague', 'vaguely', 'valley', 'valuable', 'value', 'values', 'vampire', 'vampires', 'van', 'variety', 'various', 'vast', 'vegas', 'vehicle', 'vengeance', 'verhoeven', 'version', 'versions', 'versus', 'veteran', 'vhs', 'via', 'vice', 'vicious', 'victim', 'victims', 'victor', 'victoria', 'video', 'videos', 'vietnam', 'view', 'viewed', 'viewer', 'viewers', 'viewing', 'viewings', 'views', 'village', 'villain', 'villains', 'vincent', 'violence', 'violent', 'virgin', 'virginia', 'virtually', 'virus', 'visible', 'vision', 'visions', 'visit', 'visual', 'visually', 'visuals', 'vivid', 'vocal', 'voice', 'voiced', 'voices', 'voight', 'von', 'vote', 'vs', 'vulnerable', 'wacky', 'wagner', 'wait', 'waited', 'waiting', 'waitress', 'wake', 'walk', 'walked', 'walken', 'walker', 'walking', 'walks', 'wall', 'wallace', 'walls', 'walsh', 'walter', 'wandering', 'wanna', 'wannabe', 'want', 'wanted', 'wanting', 'wants', 'war', 'ward', 'warm', 'warming', 'warmth', 'warn', 'warned', 'warner', 'warning', 'warren', 'warrior', 'warriors', 'wars', 'washington', 'waste', 'wasted', 'wasting', 'watch', 'watchable', 'watched', 'watches', 'watching', 'water', 'waters', 'watson', 'wave', 'waves', 'way', 'wayne', 'ways', 'weak', 'weakest', 'wealth', 'wealthy', 'weapon', 'weapons', 'wear', 'wearing', 'wears', 'weather', 'web', 'website', 'wedding', 'week', 'weekend', 'weeks', 'weight', 'weird', 'welcome', 'well', 'welles', 'wells', 'wendigo', 'wendy', 'went', 'werewolf', 'wes', 'west', 'western', 'westerns', 'wet', 'whale', 'whatever', 'whats', 'whatsoever', 'whenever', 'whereas', 'whether', 'whilst', 'white', 'whoever', 'whole', 'wholly', 'whoopi', 'whose', 'wicked', 'wide', 'widely', 'widmark', 'widow', 'wife', 'wild', 'wilderness', 'william', 'williams', 'willie', 'willing', 'willis', 'wilson', 'win', 'wind', 'window', 'winds', 'wing', 'winner', 'winning', 'wins', 'winter', 'winters', 'wisdom', 'wise', 'wish', 'wished', 'wishes', 'wishing', 'wit', 'witch', 'witches', 'within', 'without', 'witness', 'witnesses', 'witty', 'wives', 'wizard', 'wolf', 'woman', 'women', 'wonder', 'wondered', 'wonderful', 'wonderfully', 'wondering', 'wonderland', 'wonders', 'wont', 'wood', 'wooden', 'woods', 'woody', 'word', 'words', 'wore', 'work', 'worked', 'worker', 'workers', 'working', 'works', 'world', 'worlds', 'worn', 'worried', 'worry', 'worse', 'worst', 'worth', 'worthless', 'worthwhile', 'worthy', 'would', 'wound', 'wounded', 'wow', 'wrap', 'wrapped', 'wreck', 'wrestling', 'write', 'writer', 'writers', 'writes', 'writing', 'written', 'wrong', 'wrote', 'ww', 'wwii', 'ya', 'yard', 'yeah', 'year', 'years', 'yelling', 'yellow', 'yes', 'yesterday', 'yet', 'yeti', 'york', 'young', 'younger', 'youth', 'zane', 'zero', 'zombie', 'zombies', 'zone', 'zorro']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum up the counts of each vocabulary word\n",
        "dist = np.sum(train_data_features, axis=0)\n",
        "\n",
        "# print the vocabulary word and the number of times it \n",
        "# appears in the training set\n",
        "for tag, count in zip(vocab, dist):\n",
        "    print(count, tag)"
      ],
      "metadata": {
        "id": "zxFEb0VE-EwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning and Text Preprocessing for Test Data"
      ],
      "metadata": {
        "id": "EFSWgYmF_GNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty list and append the clean reviews one by one\n",
        "num_reviews = len(X_test[\"review\"])\n",
        "clean_test_reviews = [] \n",
        "\n",
        "print(\"Cleaning and parsing the test set movie reviews...\\n\")\n",
        "for i in range(0,num_reviews):\n",
        "    if( (i+1) % 1000 == 0 ):\n",
        "        print(\"Review %d of %d\\n\" % (i+1, num_reviews))\n",
        "    clean_review = review_to_words( X_test[\"review\"].iloc[i] )\n",
        "    clean_test_reviews.append( clean_review )\n",
        "\n",
        "# Get a bag of words for the test set, and convert to a numpy array\n",
        "test_data_features = vectorizer.transform(clean_test_reviews)\n",
        "test_data_features = test_data_features.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX9OOSsH-IbL",
        "outputId": "ff97c7b4-2302-4099-90d2-925cfa00bab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning and parsing the test set movie reviews...\n",
            "\n",
            "Review 1000 of 10000\n",
            "\n",
            "Review 2000 of 10000\n",
            "\n",
            "Review 3000 of 10000\n",
            "\n",
            "Review 4000 of 10000\n",
            "\n",
            "Review 5000 of 10000\n",
            "\n",
            "Review 6000 of 10000\n",
            "\n",
            "Review 7000 of 10000\n",
            "\n",
            "Review 8000 of 10000\n",
            "\n",
            "Review 9000 of 10000\n",
            "\n",
            "Review 10000 of 10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "mflXOula-x3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training the Random Forest...\")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize a Random Forest classifier with 150 trees\n",
        "forest = RandomForestClassifier(n_estimators=150)\n",
        "\n",
        "# Fit the forest to the training set, using the bag of words as \n",
        "# features and the sentiment labels as the response variable\n",
        "\n",
        "# This may take a few minutes to run\n",
        "forest = forest.fit( train_data_features, y_train )\n",
        "\n",
        "print(\"Done training RF\")\n",
        "\n",
        "# Use the random forest to make sentiment label predictions\n",
        "result = forest.predict(test_data_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk3zQ3vP-Gl1",
        "outputId": "2eeb78a1-a9e6-4b64-cf0a-1f197c61015e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the Random Forest...\n",
            "Done training RF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Accuracy"
      ],
      "metadata": {
        "id": "brryqDGl_QHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "\n",
        "cm = confusion_matrix(y_test, result)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "f1 = f1_score(y_test, result)\n",
        "print(\"F1 score:\")\n",
        "print(f1)\n",
        "\n",
        "print(\"Accuracy score:\")\n",
        "accuracy_score(y_test, result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu4nsTGV-MQx",
        "outputId": "669adce4-77a7-4972-c978-aed74b4edac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[4276  765]\n",
            " [ 748 4211]]\n",
            "F1 score:\n",
            "0.8477101157523905\n",
            "Accuracy score:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8487"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    }
  ]
}